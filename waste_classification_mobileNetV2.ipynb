{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Lambda, Dense, GlobalAveragePooling2D\n",
    "import tensorflow as tf\n",
    "import keras.applications.mobilenet_v2 as mobilenetv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image dimensions\n",
    "IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS = 224, 224, 3\n",
    "\n",
    "# Original Category mappings (12 categories)\n",
    "categories = {0: 'paper', 1: 'cardboard', 2: 'plastic', 3: 'metal', 4: 'trash', \n",
    "              5: 'battery', 6: 'shoes', 7: 'clothes', 8: 'green-glass', \n",
    "              9: 'brown-glass', 10: 'white-glass', 11: 'biological'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model's internal category indexing not align with the Original Category mappings\n",
    "# Reverse engineer to find the correct classification label (Plastic) - using train set plastic image\n",
    "def map_to_plastic_or_nonplastic(predicted_class):\n",
    "    # 3 is Plastic\n",
    "    if predicted_class == 3:\n",
    "        return \"Plastic\"\n",
    "    else:\n",
    "        return \"Non-Plastic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer name: dense\n",
      "    Weight name: dense, shape: <HDF5 group \"/dense/dense\" (2 members)>\n",
      "Layer name: global_average_pooling2d\n",
      "Layer name: lambda\n",
      "Layer name: mobilenetv2_1.00_224\n",
      "    Weight name: Conv1, shape: <HDF5 group \"/mobilenetv2_1.00_224/Conv1\" (1 members)>\n",
      "    Weight name: Conv_1, shape: <HDF5 group \"/mobilenetv2_1.00_224/Conv_1\" (1 members)>\n",
      "    Weight name: Conv_1_bn, shape: <HDF5 group \"/mobilenetv2_1.00_224/Conv_1_bn\" (4 members)>\n",
      "    Weight name: block_10_depthwise, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_10_depthwise\" (1 members)>\n",
      "    Weight name: block_10_depthwise_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_10_depthwise_BN\" (4 members)>\n",
      "    Weight name: block_10_expand, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_10_expand\" (1 members)>\n",
      "    Weight name: block_10_expand_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_10_expand_BN\" (4 members)>\n",
      "    Weight name: block_10_project, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_10_project\" (1 members)>\n",
      "    Weight name: block_10_project_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_10_project_BN\" (4 members)>\n",
      "    Weight name: block_11_depthwise, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_11_depthwise\" (1 members)>\n",
      "    Weight name: block_11_depthwise_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_11_depthwise_BN\" (4 members)>\n",
      "    Weight name: block_11_expand, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_11_expand\" (1 members)>\n",
      "    Weight name: block_11_expand_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_11_expand_BN\" (4 members)>\n",
      "    Weight name: block_11_project, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_11_project\" (1 members)>\n",
      "    Weight name: block_11_project_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_11_project_BN\" (4 members)>\n",
      "    Weight name: block_12_depthwise, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_12_depthwise\" (1 members)>\n",
      "    Weight name: block_12_depthwise_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_12_depthwise_BN\" (4 members)>\n",
      "    Weight name: block_12_expand, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_12_expand\" (1 members)>\n",
      "    Weight name: block_12_expand_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_12_expand_BN\" (4 members)>\n",
      "    Weight name: block_12_project, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_12_project\" (1 members)>\n",
      "    Weight name: block_12_project_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_12_project_BN\" (4 members)>\n",
      "    Weight name: block_13_depthwise, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_13_depthwise\" (1 members)>\n",
      "    Weight name: block_13_depthwise_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_13_depthwise_BN\" (4 members)>\n",
      "    Weight name: block_13_expand, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_13_expand\" (1 members)>\n",
      "    Weight name: block_13_expand_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_13_expand_BN\" (4 members)>\n",
      "    Weight name: block_13_project, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_13_project\" (1 members)>\n",
      "    Weight name: block_13_project_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_13_project_BN\" (4 members)>\n",
      "    Weight name: block_14_depthwise, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_14_depthwise\" (1 members)>\n",
      "    Weight name: block_14_depthwise_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_14_depthwise_BN\" (4 members)>\n",
      "    Weight name: block_14_expand, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_14_expand\" (1 members)>\n",
      "    Weight name: block_14_expand_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_14_expand_BN\" (4 members)>\n",
      "    Weight name: block_14_project, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_14_project\" (1 members)>\n",
      "    Weight name: block_14_project_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_14_project_BN\" (4 members)>\n",
      "    Weight name: block_15_depthwise, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_15_depthwise\" (1 members)>\n",
      "    Weight name: block_15_depthwise_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_15_depthwise_BN\" (4 members)>\n",
      "    Weight name: block_15_expand, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_15_expand\" (1 members)>\n",
      "    Weight name: block_15_expand_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_15_expand_BN\" (4 members)>\n",
      "    Weight name: block_15_project, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_15_project\" (1 members)>\n",
      "    Weight name: block_15_project_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_15_project_BN\" (4 members)>\n",
      "    Weight name: block_16_depthwise, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_16_depthwise\" (1 members)>\n",
      "    Weight name: block_16_depthwise_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_16_depthwise_BN\" (4 members)>\n",
      "    Weight name: block_16_expand, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_16_expand\" (1 members)>\n",
      "    Weight name: block_16_expand_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_16_expand_BN\" (4 members)>\n",
      "    Weight name: block_16_project, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_16_project\" (1 members)>\n",
      "    Weight name: block_16_project_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_16_project_BN\" (4 members)>\n",
      "    Weight name: block_1_depthwise, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_1_depthwise\" (1 members)>\n",
      "    Weight name: block_1_depthwise_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_1_depthwise_BN\" (4 members)>\n",
      "    Weight name: block_1_expand, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_1_expand\" (1 members)>\n",
      "    Weight name: block_1_expand_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_1_expand_BN\" (4 members)>\n",
      "    Weight name: block_1_project, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_1_project\" (1 members)>\n",
      "    Weight name: block_1_project_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_1_project_BN\" (4 members)>\n",
      "    Weight name: block_2_depthwise, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_2_depthwise\" (1 members)>\n",
      "    Weight name: block_2_depthwise_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_2_depthwise_BN\" (4 members)>\n",
      "    Weight name: block_2_expand, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_2_expand\" (1 members)>\n",
      "    Weight name: block_2_expand_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_2_expand_BN\" (4 members)>\n",
      "    Weight name: block_2_project, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_2_project\" (1 members)>\n",
      "    Weight name: block_2_project_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_2_project_BN\" (4 members)>\n",
      "    Weight name: block_3_depthwise, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_3_depthwise\" (1 members)>\n",
      "    Weight name: block_3_depthwise_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_3_depthwise_BN\" (4 members)>\n",
      "    Weight name: block_3_expand, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_3_expand\" (1 members)>\n",
      "    Weight name: block_3_expand_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_3_expand_BN\" (4 members)>\n",
      "    Weight name: block_3_project, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_3_project\" (1 members)>\n",
      "    Weight name: block_3_project_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_3_project_BN\" (4 members)>\n",
      "    Weight name: block_4_depthwise, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_4_depthwise\" (1 members)>\n",
      "    Weight name: block_4_depthwise_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_4_depthwise_BN\" (4 members)>\n",
      "    Weight name: block_4_expand, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_4_expand\" (1 members)>\n",
      "    Weight name: block_4_expand_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_4_expand_BN\" (4 members)>\n",
      "    Weight name: block_4_project, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_4_project\" (1 members)>\n",
      "    Weight name: block_4_project_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_4_project_BN\" (4 members)>\n",
      "    Weight name: block_5_depthwise, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_5_depthwise\" (1 members)>\n",
      "    Weight name: block_5_depthwise_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_5_depthwise_BN\" (4 members)>\n",
      "    Weight name: block_5_expand, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_5_expand\" (1 members)>\n",
      "    Weight name: block_5_expand_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_5_expand_BN\" (4 members)>\n",
      "    Weight name: block_5_project, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_5_project\" (1 members)>\n",
      "    Weight name: block_5_project_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_5_project_BN\" (4 members)>\n",
      "    Weight name: block_6_depthwise, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_6_depthwise\" (1 members)>\n",
      "    Weight name: block_6_depthwise_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_6_depthwise_BN\" (4 members)>\n",
      "    Weight name: block_6_expand, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_6_expand\" (1 members)>\n",
      "    Weight name: block_6_expand_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_6_expand_BN\" (4 members)>\n",
      "    Weight name: block_6_project, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_6_project\" (1 members)>\n",
      "    Weight name: block_6_project_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_6_project_BN\" (4 members)>\n",
      "    Weight name: block_7_depthwise, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_7_depthwise\" (1 members)>\n",
      "    Weight name: block_7_depthwise_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_7_depthwise_BN\" (4 members)>\n",
      "    Weight name: block_7_expand, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_7_expand\" (1 members)>\n",
      "    Weight name: block_7_expand_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_7_expand_BN\" (4 members)>\n",
      "    Weight name: block_7_project, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_7_project\" (1 members)>\n",
      "    Weight name: block_7_project_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_7_project_BN\" (4 members)>\n",
      "    Weight name: block_8_depthwise, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_8_depthwise\" (1 members)>\n",
      "    Weight name: block_8_depthwise_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_8_depthwise_BN\" (4 members)>\n",
      "    Weight name: block_8_expand, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_8_expand\" (1 members)>\n",
      "    Weight name: block_8_expand_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_8_expand_BN\" (4 members)>\n",
      "    Weight name: block_8_project, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_8_project\" (1 members)>\n",
      "    Weight name: block_8_project_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_8_project_BN\" (4 members)>\n",
      "    Weight name: block_9_depthwise, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_9_depthwise\" (1 members)>\n",
      "    Weight name: block_9_depthwise_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_9_depthwise_BN\" (4 members)>\n",
      "    Weight name: block_9_expand, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_9_expand\" (1 members)>\n",
      "    Weight name: block_9_expand_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_9_expand_BN\" (4 members)>\n",
      "    Weight name: block_9_project, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_9_project\" (1 members)>\n",
      "    Weight name: block_9_project_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/block_9_project_BN\" (4 members)>\n",
      "    Weight name: bn_Conv1, shape: <HDF5 group \"/mobilenetv2_1.00_224/bn_Conv1\" (4 members)>\n",
      "    Weight name: expanded_conv_depthwise, shape: <HDF5 group \"/mobilenetv2_1.00_224/expanded_conv_depthwise\" (1 members)>\n",
      "    Weight name: expanded_conv_depthwise_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/expanded_conv_depthwise_BN\" (4 members)>\n",
      "    Weight name: expanded_conv_project, shape: <HDF5 group \"/mobilenetv2_1.00_224/expanded_conv_project\" (1 members)>\n",
      "    Weight name: expanded_conv_project_BN, shape: <HDF5 group \"/mobilenetv2_1.00_224/expanded_conv_project_BN\" (4 members)>\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "# Open the .h5 file\n",
    "with h5py.File('waste_classification_pretrained_mobileNetv2.h5', 'r') as f:\n",
    "    # Print all layer names and their weights\n",
    "    for layer_name in f.keys():\n",
    "        print(f\"Layer name: {layer_name}\")\n",
    "        for weight_name in f[layer_name].keys():\n",
    "            print(f\"    Weight name: {weight_name}, shape: {f[layer_name][weight_name]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning – MobileNetV2 weights are frozen, and only the classification head is trained for waste classification.\n",
    "\n",
    "MobileNetV2 weights - https://www.kaggle.com/code/alexfordna/garbage-classification-mobilenetv2-92-accuracy/input?select=garbage_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\User\\anaconda3\\envs\\image-processing\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:204: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">15,372</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lambda (\u001b[38;5;33mLambda\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m)             │        \u001b[38;5;34m15,372\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,273,356</span> (8.67 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,273,356\u001b[0m (8.67 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,372</span> (60.05 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m15,372\u001b[0m (60.05 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load MobileNetV2 base model without the top layer\n",
    "mobilenetv2_layer = mobilenetv2.MobileNetV2(include_top=False, \n",
    "                                            input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS),\n",
    "                                            weights='mobilenet-v2-keras-weights/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5')\n",
    "\n",
    "mobilenetv2_layer.trainable = False\n",
    "\n",
    "\n",
    "# Create the model architecture\n",
    "model = Sequential()\n",
    "# Input layer\n",
    "model.add(tf.keras.Input(shape=(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS)))\n",
    "\n",
    "# Lambda preprocessing layer\n",
    "def mobilenetv2_preprocessing(img):\n",
    "    return mobilenetv2.preprocess_input(img)\n",
    "model.add(Lambda(mobilenetv2_preprocessing))\n",
    "\n",
    "model.add(mobilenetv2_layer)\n",
    "\n",
    "# Add the GlobalAveragePooling2D layer\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "# Add the Dense layer for classification\n",
    "model.add(Dense(12, activation='softmax'))\n",
    "\n",
    "# Load the weights from your .h5 file\n",
    "model.load_weights('waste_classification_pretrained_mobileNetv2.h5')\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No class indices found in the model.\n"
     ]
    }
   ],
   "source": [
    "if hasattr(model, 'class_indices'):\n",
    "    print(\"Class indices found:\", model.class_indices)\n",
    "else:\n",
    "    print(\"No class indices found in the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Resize the image to 224x224\n",
    "    image_resized = cv2.resize(image, (224, 224))\n",
    "    \n",
    "    # Convert image to float32 and normalize using MobileNetV2's preprocess_input\n",
    "    image_normalized = mobilenetv2.preprocess_input(image_resized.astype(np.float32))\n",
    "    \n",
    "    # Add a batch dimension (the model expects input shape: (batch_size, 224, 224, 3))\n",
    "    image_batch = np.expand_dims(image_normalized, axis=0)\n",
    "    \n",
    "    return image_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_image(model, image_path):\n",
    "    # Preprocess the image\n",
    "    image_batch = preprocess_image(image_path)\n",
    "    \n",
    "    # Perform inference\n",
    "    predictions = model.predict(image_batch)\n",
    "    \n",
    "    # Get the predicted class (argmax - the index of the highest probability)\n",
    "    predicted_class = np.argmax(predictions, axis=1)[0]\n",
    "    \n",
    "    # Map the predicted class to plastic or non-plastic before returning\n",
    "    return map_to_plastic_or_nonplastic(predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "The object is classified as: Plastic\n"
     ]
    }
   ],
   "source": [
    "# image_path = 'plastic_bottle_out_of_shape.jpg'\n",
    "# image_path = 'plastic_bottle_white_background.jpg'\n",
    "image_path = 'plastic146_in_dataset.jpg' \n",
    "# image_path = 'plastic_bottle.jpg'\n",
    "\n",
    "result = classify_image(model, image_path)\n",
    "\n",
    "print(f\"The object is classified as: {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image-processing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
